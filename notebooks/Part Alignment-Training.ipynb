{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:13:52.772120Z",
     "start_time": "2023-11-28T11:13:52.766205Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "grasp_transfer_path = os.getenv(\"GRASP_TRANSFER_SOURCE_DIR\")\n",
    "sys.path.insert(0,grasp_transfer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:13:55.259769Z",
     "start_time": "2023-11-28T11:13:53.017918Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataio import *\n",
    "from networks import *\n",
    "from torch.utils.data import DataLoader\n",
    "from visualize import *\n",
    "from trimesh.viewer import windowed\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sdf_estimator import *\n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "from utility import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:13:55.269587Z",
     "start_time": "2023-11-28T11:13:55.261710Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize = True\n",
    "reuse = False\n",
    "part_name = \"handle\" # \"rim\"\n",
    "with open(\"../configs/part_dataset_info.yaml\", 'r') as stream:\n",
    "    dataset_config = yaml.safe_load(stream)\n",
    "train = dataset_config['train_list']\n",
    "val = dataset_config['val_list']\n",
    "part = dataset_config['part_'+part_name]    \n",
    "output_name = \"../outputs/\"+part_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:14:12.531251Z",
     "start_time": "2023-11-28T11:13:55.270554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf_dataset = Part_SDF_Dataset(train,64,2000)\n",
    "sdf_dataset_val = Part_SDF_Dataset(val,16,500)\n",
    "\n",
    "RT = np.array(part['RT'])\n",
    "sdf_dataset.transform_all_pc_with_Rt(RT)\n",
    "sdf_dataset_val.transform_all_pc_with_Rt(RT)\n",
    "\n",
    "train_dataloader = DataLoader(sdf_dataset, shuffle=True,batch_size=16, num_workers=0, drop_last = True)\n",
    "val_dataloader = DataLoader(sdf_dataset_val, shuffle=True,batch_size=16, num_workers=0, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:14:14.924693Z",
     "start_time": "2023-11-28T11:14:12.533532Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MyNet(64,latent_dim = 128, hyper_hidden_features=256,hidden_num=128)\n",
    "model.to(device=torch.device('cuda:0'))\n",
    "optim = torch.optim.Adam([\n",
    "                {'params': model.sdf_net.parameters()},\n",
    "                {'params': model.hyper_net.parameters()},\n",
    "                {'params': model.latent_codes.parameters(), 'lr': 1e-4},\n",
    "                {'params': model.se3_refine.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.affine_tr.parameters(), 'lr': 1e-3}\n",
    "            ],\n",
    "    lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:02:37.494194Z",
     "start_time": "2023-11-28T10:02:36.191099Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualize\n",
    "\n",
    "part_points_list = list()\n",
    "mesh_list = list()\n",
    "\n",
    "for obj_ind in range(30,36):  \n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes[obj_ind])\n",
    "    obj_mesh = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[obj_ind].vertices,\n",
    "                            face_colors = np.random.rand(3) * 255,faces=sdf_dataset.meshes_transformed[obj_ind].faces)\n",
    "    obj_mesh.apply_scale(scale)\n",
    "    obj_mesh.apply_translation([0,0,-2*obj_ind])  \n",
    "\n",
    "    pc_inds, sdf_pos_inds, sdf_neg_inds= sdf_dataset.sample_within_sphere(obj_ind)\n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes[obj_ind])\n",
    "    points = np.copy(sdf_dataset.pointclouds[obj_ind][pc_inds][:])\n",
    "    points[:,2]-=2*obj_ind\n",
    "    part_points_list.append(points)\n",
    "\n",
    "    gripper_mesh =  get_gripper_simple_mesh(np.zeros((3,)), np.eye(3), 0.10, 0.10, score=1)\n",
    "    gripper_mesh.apply_scale(4)\n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                             faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_translation([0,0,-2*obj_ind])\n",
    "    zero_frame =  get_coordinate_frame_mesh(0.2)\n",
    "    zero_frame.apply_translation([0,0,-2*obj_ind])\n",
    "    mesh_list.append(obj_mesh)\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    mesh_list.append(zero_frame)\n",
    "\n",
    "scene = trimesh_show(part_points_list)\n",
    "scene.add_geometry(mesh_list)\n",
    "window = windowed.SceneViewer(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:24:31.839735Z",
     "start_time": "2023-11-28T10:02:39.606832Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tik=time.time()\n",
    "total_steps=0\n",
    "sphere_base = trimesh.primitives.Sphere(radius = 0.3,subdivisions=6)\n",
    "epochs=2000\n",
    "with tqdm(total=len(train_dataloader) * epochs) as pbar:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        RTs = model.get_refine_poses(torch.arange(0,64).cuda())[0].cpu().detach().numpy()\n",
    "        sdf_dataset.set_sampling_RT(RTs[:,:3,:4])   \n",
    "        sdf_dataset.set_curr_epoch(epoch)   \n",
    "        \n",
    "        model.train()\n",
    "        for step, (model_input, gt) in enumerate(train_dataloader):\n",
    "            start_time = time.time()\n",
    "            model_input = {key: value.cuda() for key, value in model_input.items()}\n",
    "            gt = {key: value.cuda() for key, value in gt.items()}\n",
    "\n",
    "            losses = model(model_input,gt,epoch)\n",
    "            train_loss = 0.\n",
    "            for loss_name, loss in losses.items():\n",
    "                single_loss = loss.mean()\n",
    "                if epoch %100== 0 and step==0:\n",
    "                    print(loss_name,single_loss)\n",
    "                train_loss += single_loss\n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "            optim.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optim.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=train_loss.item(), time=time.time() - start_time, epoch=epoch)\n",
    "            total_steps += 1\n",
    "            \n",
    "        if epoch>0 and epoch%400==0 and visualize:\n",
    "            try:\n",
    "                for index in range(2):\n",
    "                    subject_idx = index\n",
    "                    index = index\n",
    "                    v,t,_ = estimate_mesh_from_model(model, epoch, subject_idx, 64, 0.4)\n",
    "                    show_mesh(v,t)\n",
    "            except:\n",
    "                print(\"Data not good enough yet\")\n",
    "tok=time.time()\n",
    "print(tok-tik)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:56:10.262432Z",
     "start_time": "2023-11-28T10:56:09.933001Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for index in range(1):\n",
    "    subject_idx = index\n",
    "    index = index\n",
    "    v,t,_ = estimate_mesh_from_model(model, 1000, subject_idx, 64, 0.4)\n",
    "    show_mesh(v,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:10:19.738289Z",
     "start_time": "2023-11-28T11:10:19.517298Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_name):\n",
    "    os.mkdir(output_name)\n",
    "torch.save(model.state_dict(), output_name+\"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:14:15.030784Z",
     "start_time": "2023-11-28T11:14:14.925799Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(output_name+\"/model\"))#my_model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-27T14:03:16.645591Z",
     "start_time": "2022-07-27T14:03:16.631215Z"
    }
   },
   "source": [
    "# Validation Visulizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:56:23.151576Z",
     "start_time": "2023-11-28T10:56:22.379020Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "contour_plot(model,1000) # Change to 0 or any other intermediate value (Low Frequency Positional encodings) to see the smoother output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:20:06.816784Z",
     "start_time": "2023-11-28T11:20:06.789591Z"
    }
   },
   "outputs": [],
   "source": [
    "RTs = model.get_refine_poses(torch.arange(0,64).cuda(),affine=False).cpu().detach().numpy()\n",
    "sdf_dataset.set_sampling_RT(RTs[:,:3,:4])    \n",
    "RT_base=np.eye(4)\n",
    "RT_base[:3,:]= RTs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-24T16:39:03.280Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Interpolation Between different Embeddings\n",
    "## This creates an HTML file in directory $output_name\n",
    "for index, obj_id in enumerate([3,4,5]):\n",
    "    mesh_list=estimate_mesh_traj_from_model(model, 2000,[0,obj_ind], 50, 64)\n",
    "    draw_mesh_list(mesh_list,output_name+\"/interp_\"+str(obj_ind+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:56:36.991084Z",
     "start_time": "2023-11-28T10:56:30.455026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Object Grasp Visualization\n",
    "part_points_list = list()\n",
    "mesh_list = list()\n",
    "\n",
    "for obj_ind in range(20,30):  \n",
    "    color = np.random.rand(3) * 255\n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes[obj_ind])\n",
    "    \n",
    "    obj_mesh = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[obj_ind].vertices,\n",
    "                            face_colors = color,faces=sdf_dataset.meshes_transformed[obj_ind].faces)\n",
    "    obj_mesh.apply_scale(scale)\n",
    "    obj_mesh.apply_translation([0,0,-1.5*obj_ind])  \n",
    "\n",
    "    pc_inds,sdf_pos_inds,sdf_neg_inds= sdf_dataset.sample_within_sphere(obj_ind,radius=0.8)\n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes[obj_ind])\n",
    "    points = np.copy(sdf_dataset.pointclouds[obj_ind][pc_inds][:])\n",
    "    points[:,2]-=1.5*obj_ind\n",
    "    part_points_list.append(points)\n",
    "\n",
    "    gripper_mesh =  get_gripper_simple_mesh(np.zeros((3,)), np.eye(3), 0.10, 0.10, score=1)\n",
    "    gripper_mesh.apply_scale(4)\n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                             faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_translation([0,0,-1.5*obj_ind])\n",
    "    zero_frame =  get_coordinate_frame_mesh(0.2)\n",
    "    zero_frame.apply_translation([0,0,-1.5*obj_ind])\n",
    "    mesh_list.append(obj_mesh)\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    mesh_list.append(zero_frame)\n",
    "    \n",
    "    RT= np.eye(4)\n",
    "    RT[:3,:] = sdf_dataset.sampling_RTs[obj_ind]\n",
    "    \n",
    "    obj_mesh = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[obj_ind].vertices,\n",
    "                            face_colors = color,faces=sdf_dataset.meshes_transformed[obj_ind].faces)\n",
    "    obj_mesh.apply_scale(scale)\n",
    "    obj_mesh.apply_translation([3,0,-1.5*obj_ind])  \n",
    "\n",
    "    pc_inds,sdf_pos_inds,sdf_neg_inds= sdf_dataset.sample_within_sphere(obj_ind,RT[:3,:],radius=0.8)\n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes[obj_ind])\n",
    "    points = np.copy(sdf_dataset.pointclouds[obj_ind][pc_inds][:])\n",
    "    points[:,2]-=1.5*obj_ind\n",
    "    points[:,0]+=3\n",
    "    part_points_list.append(points)\n",
    "\n",
    "    gripper_mesh =  get_gripper_simple_mesh(np.zeros((3,)), np.eye(3), 0.10, 0.10, score=1)\n",
    "    gripper_mesh.apply_scale(4)\n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                             faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_transform(find_inverse_RT_4x4(RT))    \n",
    "    gripper_mesh_for_grasp.apply_transform(RT_base)\n",
    "    \n",
    "    gripper_mesh_for_grasp.apply_translation([3,0,-1.5*obj_ind])\n",
    "    zero_frame =  get_coordinate_frame_mesh(0.2)\n",
    "    zero_frame.apply_transform(find_inverse_RT_4x4(RT))    \n",
    "    zero_frame.apply_transform(RT_base)\n",
    "\n",
    "    zero_frame.apply_translation([3,0,-1.5*obj_ind])\n",
    "    mesh_list.append(obj_mesh)\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    mesh_list.append(zero_frame)    \n",
    "\n",
    "scene = trimesh_show(part_points_list)\n",
    "scene.add_geometry(mesh_list)\n",
    "window = windowed.SceneViewer(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T10:56:44.315439Z",
     "start_time": "2023-11-28T10:56:38.972247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Object Grasp + Reconstruction Visualization\n",
    "\n",
    "mesh_list=list()\n",
    "part_points_list=list()\n",
    "\n",
    "shape_tr = 0\n",
    "\n",
    "for index in range(20,30):\n",
    "    shape_Tr=[0,0,shape_tr*2]\n",
    "    fc_color = np.random.rand(3) * 255 \n",
    "    \n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes_transformed[index])\n",
    "    \n",
    "    mesh3 = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[index].vertices*scale, \n",
    "                            faces=sdf_dataset.meshes_transformed[index].faces,face_colors=fc_color)\n",
    "    mesh3.apply_translation(shape_Tr)  \n",
    "\n",
    "    RT= np.eye(4)\n",
    "    RT[:3,:] = sdf_dataset.sampling_RTs[index]\n",
    "    RT_inv = find_inverse_RT_4x4(RT)   \n",
    "        \n",
    "    points = estimate_points_from_model(model, 2000, index, 64,0.5)\n",
    "    points = ((RT_inv@ to_hom_np(points).T).T)[:,:3]\n",
    "    points = ((RT_base@ to_hom_np(points).T).T)[:,:3]\n",
    "    points[:,:3]+=shape_Tr\n",
    "    part_points_list.append(points)    \n",
    "        \n",
    "    mesh_list.append(mesh3)\n",
    "    \n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                         faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_transform(RT_inv)\n",
    "    gripper_mesh_for_grasp.apply_transform(RT_base)\n",
    "    gripper_mesh_for_grasp.apply_translation(shape_Tr)    \n",
    "    gripper_mesh_for_grasp.colors = np.tile(np.array([175,175,175]), (gripper_mesh_for_grasp.vertices.shape[0], 1))\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    \n",
    "    shape_tr=shape_tr+1\n",
    "    \n",
    "scene = trimesh_show_green(part_points_list)\n",
    "scene.add_geometry(mesh_list)\n",
    "#scene.show()\n",
    "\n",
    "window = windowed.SceneViewer(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:06:58.505089Z",
     "start_time": "2023-11-28T11:06:58.490293Z"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_mesh_from_model(model, epoch, subject_idx, resolution = 64, scale = 1 ):\n",
    "    with torch.no_grad():\n",
    "        N=resolution\n",
    "        max_batch=64 ** 3\n",
    "        model.eval()\n",
    "\n",
    "        # NOTE: the voxel_origin is actually the (bottom, left, down) corner, not the middle\n",
    "        voxel_origin = [-1, -1, -1]\n",
    "        voxel_size = 2.0 / (N - 1)\n",
    "\n",
    "        overall_index = torch.arange(0, N ** 3, 1, out=torch.LongTensor())\n",
    "        samples = torch.zeros(N ** 3, 4)\n",
    "\n",
    "        # transform first 3 columns\n",
    "        # to be the x, y, z index\n",
    "        samples[:, 2] = overall_index % N\n",
    "        samples[:, 1] = (overall_index.long() / N) % N\n",
    "        samples[:, 0] = ((overall_index.long() / N) / N) % N\n",
    "\n",
    "        # transform first 3 columns\n",
    "        # to be the x, y, z coordinate\n",
    "        samples[:, 0] = (samples[:, 0] * scale * voxel_size) + scale *voxel_origin[2]\n",
    "        samples[:, 1] = (samples[:, 1] * scale * voxel_size) + scale *voxel_origin[1]\n",
    "        samples[:, 2] = (samples[:, 2] * scale * voxel_size) + scale *voxel_origin[0]\n",
    "\n",
    "        num_samples = N ** 3\n",
    "\n",
    "        samples.requires_grad = False\n",
    "\n",
    "        head = 0\n",
    "        if subject_idx==-1:\n",
    "            subject_idx = torch.Tensor([range(64)]).squeeze().long().cuda()[None,...]\n",
    "            embedding = model.get_latent_code(subject_idx)\n",
    "            embedding=embedding.mean(dim=1)\n",
    "        else:\n",
    "            subject_idx = torch.Tensor([subject_idx]).squeeze().long().cuda()[None,...]\n",
    "            embedding = model.get_latent_code(subject_idx)\n",
    "        while head < num_samples:\n",
    "            sample_subset = samples[head : min(head + max_batch, num_samples), 0:3].cuda()[None,...]\n",
    "            samples[head : min(head + max_batch, num_samples), 3] = (\n",
    "                model.inference(sample_subset,embedding,epoch)['model_out']\n",
    "                .squeeze()#.squeeze(1)\n",
    "                .detach()\n",
    "                .cpu()\n",
    "            )\n",
    "            head += max_batch\n",
    "\n",
    "        sdf_values = samples[:, 3]\n",
    "        sdf_values = sdf_values.reshape(N, N, N)\n",
    "        v,t,n,_ = measure.marching_cubes(sdf_values.numpy(),0.01,step_size = 1,spacing=[2/N,2/N,2/N])\n",
    "        return scale*(v-1),t,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:21:07.715401Z",
     "start_time": "2023-11-28T11:20:50.147890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi Object Grasp + Reconstruction Visualization THREE-VIEW\n",
    "# Reconstruction - Before Alignment - After Alignment\n",
    "mesh_list=list()\n",
    "sphere_base = trimesh.primitives.Sphere(radius = 0.5,subdivisions=6)\n",
    "sphere_base2 = trimesh.primitives.Sphere(radius = 0.1,subdivisions=3)\n",
    "\n",
    "shape_tr = 0\n",
    "for index in range(0,8):\n",
    "    fc_color = np.random.rand(3) * 255\n",
    "    v,t,_ = estimate_mesh_from_model(model, 1000, index, 64, scale=0.5)\n",
    "\n",
    "    mesh = trimesh.Trimesh(vertices=v, faces=t, face_colors=fc_color)\n",
    "    mesh.apply_translation([-2.5, 0, shape_tr*2.5])   \n",
    "    \n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset.meshes_transformed[index])\n",
    "\n",
    "    RT= np.eye(4)\n",
    "    RT[:3,:] = sdf_dataset.sampling_RTs[index]\n",
    "    RT_inv = find_inverse_RT_4x4(RT)\n",
    "    mesh2 = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[index].vertices*scale,\n",
    "                            face_colors=fc_color, faces=sdf_dataset.meshes_transformed[index].faces)\n",
    "    mesh2.apply_translation([0, 0, shape_tr*2.5])\n",
    "    \n",
    "    mesh3 = trimesh.Trimesh(vertices=sdf_dataset.meshes_transformed[index].vertices*scale, \n",
    "                            faces=sdf_dataset.meshes_transformed[index].faces,face_colors=fc_color)\n",
    "\n",
    "    mesh3.apply_translation([3, 0, shape_tr*2.5])  \n",
    "    mesh_list.append(mesh)\n",
    "    mesh_list.append(mesh2)\n",
    "    mesh_list.append(mesh3)\n",
    "    gripper_mesh =  get_gripper_simple_mesh(np.zeros((3,)), np.eye(3), 0.10, 0.10, score=1)\n",
    "    gripper_mesh.apply_scale(4)\n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                         faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_translation([0, 0, shape_tr*2.5])\n",
    "    gripper_mesh_for_grasp.colors = np.tile(np.array([175,175,175]), (gripper_mesh_for_grasp.vertices.shape[0], 1))\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    \n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                         faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_transform(RT_inv)\n",
    "    gripper_mesh_for_grasp.apply_translation([3, 0, shape_tr*2.5])\n",
    "    gripper_mesh_for_grasp.colors = np.tile(np.array([175,175,175]), (gripper_mesh_for_grasp.vertices.shape[0], 1))\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    \n",
    "    shape_tr=shape_tr+1\n",
    "    \n",
    "scene = trimesh.Scene()\n",
    "scene.add_geometry(mesh_list)\n",
    "window = windowed.SceneViewer(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:12:09.910136Z",
     "start_time": "2023-11-28T11:12:09.899553Z"
    }
   },
   "outputs": [],
   "source": [
    "model_transfer = MyNetTransfer(16, model, latent_dim = 128)\n",
    "model_transfer.to(device=torch.device('cuda:0'))\n",
    "optim = torch.optim.Adam([\n",
    "                {'params': model_transfer.latent_codes.parameters(), 'lr': 1e-3},\n",
    "                {'params': model_transfer.se3_refine.parameters(), 'lr': 1e-3},\n",
    "                {'params': model_transfer.affine_tr.parameters(), 'lr': 1e-3}\n",
    "            ],\n",
    "    lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:12:24.910840Z",
     "start_time": "2023-11-28T11:12:10.436115Z"
    }
   },
   "outputs": [],
   "source": [
    "total_steps=0\n",
    "sphere_base = trimesh.primitives.Sphere(radius = 0.3,subdivisions=6)\n",
    "epochs=100\n",
    "with tqdm(total=len(val_dataloader) * epochs) as pbar:\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        RTs,_ = model_transfer.get_refine_poses(torch.arange(0,16).cuda())\n",
    "        RTs = RTs.cpu().detach().numpy()\n",
    "        sdf_dataset_val.set_sampling_RT(RTs[:,:3,:4])        \n",
    "        model_transfer.train()\n",
    "        for step, (model_input, gt) in enumerate(val_dataloader):\n",
    "            start_time = time.time()\n",
    "            model_input = {key: value.cuda() for key, value in model_input.items()}\n",
    "            gt = {key: value.cuda() for key, value in gt.items()}\n",
    "\n",
    "            losses = model_transfer(model_input,gt,1000)\n",
    "\n",
    "            train_loss = 0.\n",
    "            for loss_name, loss in losses.items():\n",
    "                single_loss = loss.mean()\n",
    "                train_loss += single_loss\n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "            optim.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optim.step()\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=train_loss, time=time.time() - start_time, epoch=epoch)\n",
    "            total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T11:13:01.403583Z",
     "start_time": "2023-11-28T11:12:24.914015Z"
    }
   },
   "outputs": [],
   "source": [
    "mesh_list=list()\n",
    "sphere_base = trimesh.primitives.Sphere(radius = 0.5,subdivisions=6)\n",
    "sphere_base2 = trimesh.primitives.Sphere(radius = 0.1,subdivisions=3)\n",
    "\n",
    "shape_tr = 0\n",
    "for index in range(0,16): \n",
    "    fc_color = np.random.rand(3) * 255\n",
    "    v,t,_ = estimate_mesh_from_model(model_transfer, 1000, index, 64, 0.5)\n",
    "    mesh = trimesh.Trimesh(vertices=v, faces=t, face_colors=fc_color)\n",
    "    mesh.apply_translation([-2+shape_tr//2*4.5, -0.2, shape_tr%2*2.5])   \n",
    "\n",
    "    _,scale = compute_unit_sphere_transform(sdf_dataset_val.meshes_transformed[index])\n",
    "\n",
    "    mesh2 = trimesh.Trimesh(vertices=sdf_dataset_val.meshes_transformed[index].vertices*scale,\n",
    "                            face_colors=fc_color, faces=sdf_dataset_val.meshes_transformed[index].faces)\n",
    "    mesh2.apply_translation([shape_tr//2*4.5, 0, shape_tr%2*2.5])\n",
    "    \n",
    "    RT= np.eye(4)\n",
    "    RT[:3,:] = sdf_dataset_val.sampling_RTs[index]\n",
    "    RT_inv = find_inverse_RT_4x4(RT)\n",
    "    mesh_list.append(mesh)\n",
    "    mesh_list.append(mesh2)\n",
    "    \n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                         faces=gripper_mesh.faces,face_colors=[175,175,175,75])\n",
    "    \n",
    "    gripper_mesh_for_grasp.apply_translation([shape_tr//2*4.5, 0, shape_tr%2*2.5])\n",
    "    gripper_mesh_for_grasp.colors = np.tile(np.array([175,175,175]), (gripper_mesh_for_grasp.vertices.shape[0], 1))\n",
    "    \n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    \n",
    "    gripper_mesh_for_grasp = trimesh.Trimesh(vertices=gripper_mesh.vertices,\n",
    "                                         faces=gripper_mesh.faces,face_colors=[175,175,175])\n",
    "    gripper_mesh_for_grasp.apply_transform(RT_inv)\n",
    "    gripper_mesh_for_grasp.apply_translation([shape_tr//2*4.5, 0, shape_tr%2*2.5])\n",
    "    gripper_mesh_for_grasp.colors = np.tile(np.array([175,175,175]), (gripper_mesh_for_grasp.vertices.shape[0], 1))\n",
    "    mesh_list.append(gripper_mesh_for_grasp)\n",
    "    \n",
    "    shape_tr=shape_tr+1\n",
    "    \n",
    "scene = trimesh.Scene()\n",
    "scene.add_geometry(mesh_list)\n",
    "window = windowed.SceneViewer(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grasp_transfer_venv",
   "language": "python",
   "name": "grasp_transfer_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
